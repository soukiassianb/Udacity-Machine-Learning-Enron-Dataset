{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import division\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../tools/\")\n",
    "import pickle\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# features selected\n",
    "features_list = ['poi',\n",
    "                 'bonus',\n",
    "                 'salary',\n",
    "                 'deferral_payments',\n",
    "                 'deferred_income',\n",
    "                 'director_fees',\n",
    "                 'exercised_stock_options',\n",
    "                 'expenses',\n",
    "                 'from_messages',\n",
    "                 'from_poi_to_this_person',\n",
    "                 'from_this_person_to_poi',\n",
    "                 'loan_advances',\n",
    "                 'long_term_incentive',\n",
    "                 'other', \n",
    "                 'restricted_stock',\n",
    "                 'restricted_stock_deferred', \n",
    "                 'shared_receipt_with_poi',\n",
    "                 'to_messages', \n",
    "                 'total_payments', \n",
    "                 'total_stock_value']\n",
    "\n",
    "# Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bonus': 'NaN',\n",
       " 'deferral_payments': 'NaN',\n",
       " 'deferred_income': 'NaN',\n",
       " 'director_fees': 'NaN',\n",
       " 'email_address': 'NaN',\n",
       " 'exercised_stock_options': 'NaN',\n",
       " 'expenses': 'NaN',\n",
       " 'from_messages': 'NaN',\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'loan_advances': 'NaN',\n",
       " 'long_term_incentive': 'NaN',\n",
       " 'other': 362096,\n",
       " 'poi': False,\n",
       " 'restricted_stock': 'NaN',\n",
       " 'restricted_stock_deferred': 'NaN',\n",
       " 'salary': 'NaN',\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'to_messages': 'NaN',\n",
       " 'total_payments': 362096,\n",
       " 'total_stock_value': 'NaN'}"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove outliers\n",
    "data_dict.pop('TOTAL', 0)\n",
    "data_dict.pop('LOCKHART EUGENE E', 0)\n",
    "data_dict.pop('THE TRAVEL AGENCY IN THE PARK', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We load the dataset into a dataframe \n",
    "# and compute total for payments and stock values\n",
    "# We'll then compare the results with the provided financial pdf file.\n",
    "\n",
    "df = pd.DataFrame.from_dict(data_dict).T\n",
    "df.replace('NaN', np.nan, inplace=True)\n",
    "\n",
    "payment_total_fields = ['salary',\n",
    "                        'bonus', \n",
    "                        'long_term_incentive', \n",
    "                        'deferred_income',\n",
    "                        'deferral_payments',\n",
    "                        'loan_advances',\n",
    "                        'other',\n",
    "                        'expenses',\n",
    "                        'director_fees']\n",
    "\n",
    "total_stock_value_fields = ['exercised_stock_options',\n",
    "                            'restricted_stock',\n",
    "                            'restricted_stock_deferred',]\n",
    "\n",
    "with pd.option_context('display.max_rows', 999, 'display.max_columns', 3):\n",
    "    # Set to true to print the results\n",
    "    if False:\n",
    "        print(df[payment_total_fields].sum(axis=1))\n",
    "    if False:\n",
    "        print(df[total_stock_value_fields].sum(axis=1))\n",
    "        \n",
    "# After printing the results and comparing with the file\n",
    "# BELFER ROBERT and BHATNAGAR SANJAY appeared to have incorrect total values\n",
    "# so I updated the values within the dataset using the ones on the pdf file.\n",
    "data_dict['BELFER ROBERT']['deferred_income'] = -102500\n",
    "data_dict['BELFER ROBERT']['deferral_payments'] = 0\n",
    "data_dict['BELFER ROBERT']['expenses'] = 3285\n",
    "data_dict['BELFER ROBERT']['director_fees'] = 102500\n",
    "data_dict['BELFER ROBERT']['total_payments'] = 3285\n",
    "data_dict['BELFER ROBERT']['exercised_stock_options'] = 0\n",
    "data_dict['BELFER ROBERT']['restricted_stock'] = 44093\n",
    "data_dict['BELFER ROBERT']['restricted_stock_deferred'] = -44093\n",
    "data_dict['BELFER ROBERT']['total_stock_value'] = 0\n",
    "\n",
    "data_dict['BHATNAGAR SANJAY']['total_payments'] = 137864\n",
    "data_dict['BHATNAGAR SANJAY']['expenses'] = 137864\n",
    "data_dict['BHATNAGAR SANJAY']['other'] = 0\n",
    "data_dict['BHATNAGAR SANJAY']['director_fees'] = 0\n",
    "data_dict['BHATNAGAR SANJAY']['exercised_stock_options'] = 15456290\n",
    "data_dict['BHATNAGAR SANJAY']['restricted_stock'] = 2604490\n",
    "data_dict['BHATNAGAR SANJAY']['restricted_stock_deferred'] = -2604490\n",
    "data_dict['BHATNAGAR SANJAY']['total_stock_value'] = 15456290"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create new features\n",
    "for v in data_dict.values():\n",
    "    from_poi_to_this_person = v[\"from_poi_to_this_person\"]\n",
    "    to_messages = v[\"to_messages\"]\n",
    "    from_this_person_to_poi = v[\"from_this_person_to_poi\"]\n",
    "    from_messages = v[\"from_messages\"]\n",
    "\n",
    "    v[\"from_poi_ratio\"] = (float(from_poi_to_this_person) / float(to_messages) if\n",
    "                           to_messages not in [0, \"NaN\"] and from_poi_to_this_person\n",
    "                           not in [0, \"NaN\"] else 0.0)\n",
    "    v[\"to_poi_ratio\"] = (float(from_this_person_to_poi) / float(from_messages) if\n",
    "                         from_messages not in [0, \"NaN\"] and from_this_person_to_poi\n",
    "                         not in [0, \"NaN\"] else 0.0)\n",
    "\n",
    "features_list.append(\"from_poi_ratio\")\n",
    "features_list.append(\"to_poi_ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "# Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys=True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We use StratifiedShuffleSplit because the dataset is small and unbalanced\n",
    "cv = StratifiedShuffleSplit(labels, n_iter=100, test_size=0.75, random_state = 42)\n",
    "for train_index, test_index in cv:\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    for ii in train_index:\n",
    "        X_train.append( features[ii] )\n",
    "        y_train.append( labels[ii] )\n",
    "    for jj in test_index:\n",
    "        X_test.append( features[jj] )\n",
    "        y_test.append( labels[jj] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__weights': 'distance', 'clf__algorithm': 'kd_tree', 'feature_selection__pca__n_components': 2, 'feature_selection__kbest__k': 2, 'clf__n_neighbors': 2}\n",
      "['__abstractmethods__', '__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_cache', '_abc_negative_cache', '_abc_negative_cache_version', '_abc_registry', '_estimator_type', '_fit', '_get_param_names', 'best_estimator_', 'best_params_', 'best_score_', 'cv', 'decision_function', 'error_score', 'estimator', 'fit', 'fit_params', 'get_params', 'grid_scores_', 'iid', 'inverse_transform', 'n_jobs', 'param_grid', 'pre_dispatch', 'predict', 'predict_log_proba', 'predict_proba', 'refit', 'score', 'scorer_', 'scoring', 'set_params', 'transform', 'verbose']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_selection = FeatureUnion([\n",
    "        ('kbest', SelectKBest(f_classif)), \n",
    "        ('pca', PCA())\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('feature_selection', feature_selection), \n",
    "        #('clf', RandomForestClassifier())\n",
    "        #('clf', AdaBoostClassifier())\n",
    "        #('clf', GradientBoostingClassifier())\n",
    "        #('clf', SVC())\n",
    "        #('clf', DecisionTreeClassifier())\n",
    "        ('clf', KNeighborsClassifier(weights='distance', algorithm='ball_tree'))\n",
    "    ])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, {\n",
    "        'feature_selection__kbest__k': [2, 3, 4, 5, 7, 10],\n",
    "        'feature_selection__pca__n_components':[2, 5, 10,],\n",
    "        \n",
    "        # ADABOOST\n",
    "        #'clf__algorithm' : ['SAMME', 'SAMME.R'],\n",
    "        #'clf__n_estimators': [25, 50, 100],\n",
    "        #'clf__learning_rate': [.5, 1., 1.5],\n",
    "        \n",
    "        # GRADIENT BOOSTING\n",
    "        #'clf__loss' : ['deviance', 'exponential'],\n",
    "        #'clf__learning_rate': [0.1, 0.3, 0.5],\n",
    "        #'clf__n_estimators': [25, 50, 100],\n",
    "        #'clf__max_depth': [3, 5, 10],\n",
    "        #'clf__min_samples_split': [1, 3, 5, 10],\n",
    "        \n",
    "        # SVM\n",
    "        #'clf__kernel': [ 'sigmoid', 'poly','rbf'],\n",
    "        #'clf__C': [1, 5, 10, 20, 200, 1000],\n",
    "        #'clf__class_weight' :[None, 'balanced'],\n",
    "        \n",
    "        # RANDOM FOREST\n",
    "        #'clf__n_estimators': [25, 30, 50, 80, 100],\n",
    "        #'clf__min_samples_split': [1, 3, 5, 10],\n",
    "        #'clf__criterion': ['gini', 'entropy'],\n",
    "        #'clf__max_depth': [3, 6, 8, 11, 15, 20]\n",
    "        \n",
    "        # KNC\n",
    "        'clf__n_neighbors': [2, 4, 6, 10],\n",
    "        'clf__weights': ['distance', 'uniform'],\n",
    "        'clf__algorithm': ['kd_tree', 'ball_tree', 'auto', 'brute'],\n",
    "\n",
    "    }, scoring='recall')\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "clf = pipeline.set_params(**grid_search.best_params_)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(dir(grid_search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.95      0.83      0.89        94\n",
      "        1.0       0.38      0.71      0.50        14\n",
      "\n",
      "avg / total       0.88      0.81      0.84       108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, clf.predict(X_test))\n",
    "print report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dump classifier and dta\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from_poi_to_this_person: 21.255200408\n",
      "bonus: 10.0612709723\n",
      "deferred_income: 10.0463988887\n",
      "loan_advances: nan\n",
      "to_poi_ratio: 8.43088861423\n",
      "shared_receipt_with_poi: 7.69380475908\n",
      "to_messages: 7.37776156125\n",
      "deferral_payments: 6.59293160021\n",
      "total_stock_value: 5.33281200519\n",
      "exercised_stock_options: 4.80842753688\n",
      "restricted_stock: 3.99888294796\n",
      "total_payments: 3.97103040479\n",
      "from_this_person_to_poi: 2.99587978817\n",
      "from_poi_ratio: 1.80064309494\n",
      "salary: 1.58172487906\n",
      "long_term_incentive: 1.01950117319\n",
      "expenses: 0.225226682401\n",
      "restricted_stock_deferred: 0.184596363914\n",
      "director_fees: 0.165144626037\n",
      "from_messages: 0.0230553897022\n",
      "other: 0.0181436652867\n"
     ]
    }
   ],
   "source": [
    "# Getting the feature Scores\n",
    "k = grid_search.get_params(True)['estimator__feature_selection__transformer_list'][0][1]\n",
    "features_scores = zip(features_list[1:], k.scores_)\n",
    "for f, s in sorted(features_scores, key=lambda x: x[1], reverse=True):\n",
    "    print('%s: %s'%(f, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
